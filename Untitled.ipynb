{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb029c71-d267-403c-a28a-d5557ea740e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/huggingface/diffusers@@shap-ee\n",
      "  Cloning git://github.com/huggingface/diffusers@ (to revision shap-ee) to c:\\users\\musta\\appdata\\local\\temp\\pip-req-build-dnkafm6f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet git://github.com/huggingface/diffusers@ 'C:\\Users\\musta\\AppData\\Local\\Temp\\pip-req-build-dnkafm6f'\n",
      "  fatal: unable to connect to github.com:\n",
      "  github.com[0: 140.82.116.4]: errno=Unknown error\n",
      "\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  git clone --filter=blob:none --quiet git://github.com/huggingface/diffusers@ 'C:\\Users\\musta\\AppData\\Local\\Temp\\pip-req-build-dnkafm6f' did not run successfully.\n",
      "  exit code: 128\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "git clone --filter=blob:none --quiet git://github.com/huggingface/diffusers@ 'C:\\Users\\musta\\AppData\\Local\\Temp\\pip-req-build-dnkafm6f' did not run successfully.\n",
      "exit code: 128\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers accelerate trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ddcb5b1-a2d7-460f-b096-585047c3db88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap_e'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshap_e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample_latents\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshap_e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgaussian_diffusion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m diffusion_from_config\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshap_e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model, load_config\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shap_e'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e0514b-120b-420b-988c-ab395566ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbf7f5b-fee1-4e52-b76f-276232531490",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = load_model('transmitter', device=device) \n",
    "model = load_model('text300M', device=device) \n",
    "diffusion = diffusion_from_config(load_config('diffusion')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5d02fd-14e6-4142-9ccb-1d5a169e1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1 # this is the size of the models, higher values take longer to generate.\n",
    "# guidance_scale = 15.0 # this is the scale of the guidance, higher values make the model look more like the prompt.\n",
    "# prompt = \"a teddy bear\" # this is the prompt, you can change this to anything you want.\n",
    "\n",
    "# latents = sample_latents(\n",
    "#     batch_size=batch_size,\n",
    "#     model=model,\n",
    "#     diffusion=diffusion,\n",
    "#     guidance_scale=guidance_scale,\n",
    "#     model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "#     progress=True,\n",
    "#     clip_denoised=True,\n",
    "#     use_fp16=True,\n",
    "#     use_karras=True,\n",
    "#     karras_steps=64,\n",
    "#     sigma_min=1e-3,\n",
    "#     sigma_max=160,\n",
    "#     s_churn=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c195ee4-c7d8-425c-91f8-4eb21bb84c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_mode = 'nerf' # you can change this to 'stf'\n",
    "# size = 64 # this is the size of the renders, higher values take longer to render.\n",
    "\n",
    "# cameras = create_pan_cameras(size, device)\n",
    "# for i, latent in enumerate(latents):\n",
    "#     images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "#     display(gif_widget(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1886e13-d350-47a5-895a-2978d2fcc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of saving the latents as meshes.\n",
    "# from shap_e.util.notebooks import decode_latent_mesh\n",
    "\n",
    "# for i, latent in enumerate(latents):\n",
    "#     t = decode_latent_mesh(xm, latent).tri_mesh()\n",
    "#     with open(f'models/example_mesh_{i}.ply', 'wb') as f: # this is three-dimensional geometric data of model.\n",
    "#         t.write_obj(f)\n",
    "#     with open(f'models/example_mesh_{i}.obj', 'w') as f: # we will use this file to customize in Blender Studio later.\n",
    "#         t.write_obj(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d779737-1937-4298-a40c-e1240d07cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in ./env/lib/python3.10/site-packages (3.0.3)\n",
      "Requirement already satisfied: flask-cors in ./env/lib/python3.10/site-packages (4.0.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in ./env/lib/python3.10/site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./env/lib/python3.10/site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in ./env/lib/python3.10/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in ./env/lib/python3.10/site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: click>=8.1.3 in ./env/lib/python3.10/site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be210aa1-3992-4bb0-b5fc-b37d9760cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shap_e.util.notebooks import decode_latent_mesh\n",
    "\n",
    "# def get_ai_3d_model(modeldesc):\n",
    "#     batch_size = 1 # this is the size of the models, higher values take longer to generate.\n",
    "#     guidance_scale = 15.0 # this is the scale of the guidance, higher values make the model look more like the prompt.\n",
    "#     prompt = modeldesc # this is the prompt, you can change this to anything you want.\n",
    "    \n",
    "#     latents = sample_latents(\n",
    "#         batch_size=batch_size,\n",
    "#         model=model,\n",
    "#         diffusion=diffusion,\n",
    "#         guidance_scale=guidance_scale,\n",
    "#         model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "#         progress=True,\n",
    "#         clip_denoised=True,\n",
    "#         use_fp16=True,\n",
    "#         use_karras=True,\n",
    "#         karras_steps=64,\n",
    "#         sigma_min=1e-3,\n",
    "#         sigma_max=160,\n",
    "#         s_churn=0,\n",
    "#     )\n",
    "#     # Example of saving the latents as meshes.\n",
    "    \n",
    "#     for i, latent in enumerate(latents):\n",
    "#         t = decode_latent_mesh(xm, latent).tri_mesh()\n",
    "#         with open(f'models/{modeldesc}.ply', 'wb') as f: # this is three-dimensional geometric data of model.\n",
    "#             t.write_obj(f)\n",
    "#         with open(f'models/{modeldesc}.obj', 'w') as f: # we will use this file to customize in Blender Studio later.\n",
    "#             t.write_obj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1050b-827d-4bec-839c-47f4065051ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.0.0.175:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b087e591154b87b60f772c371503ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/workspace/vr_ai_world/shap-e/shap_e/models/stf/renderer.py:286: UserWarning: exception rendering with PyTorch3D: No module named 'pytorch3d'\n",
      "  warnings.warn(f\"exception rendering with PyTorch3D: {exc}\")\n",
      "/home/z/workspace/vr_ai_world/shap-e/shap_e/models/stf/renderer.py:287: UserWarning: falling back on native PyTorch renderer, which does not support full gradients\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [22/May/2024 10:59:22] \"GET /model?description=a%20chocolate%20donut HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3af6c034c4419da0351e1038bef823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2024 20:13:14] \"GET /model?description=%20A%20teddy%20bear. HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea14056f152473f87d1df252ec6bfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2024 20:14:08] \"GET /model?description=%20A%20cheeseburger. HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466eef33c3ea4b51b78dd1651bf76fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2024 20:14:32] \"GET /model?description=%20[typing] HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f5d4a3802d41daa7e5aa9467d1810c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2024 20:15:16] \"GET /model?description=%20Cheese%20pizza HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213d6824781144df841c621123cc6547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2024 21:49:09] \"GET /model?description=%20It%20trashed%20again. HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from shap_e.util.notebooks import decode_latent_mesh\n",
    "from flask import Flask, send_from_directory, request\n",
    "from flask_cors import CORS\n",
    "\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for all routes\n",
    "\n",
    "# Placeholder for your model and diffusion objects\n",
    "# model = None  # Replace with your actual model\n",
    "# diffusion = None  # Replace with your actual diffusion\n",
    "# xm = None  # Replace with your actual xm\n",
    "\n",
    "def get_ai_3d_model(modeldesc):\n",
    "    batch_size = 1\n",
    "    guidance_scale = 15.0\n",
    "    prompt = modeldesc\n",
    "\n",
    "    latents = sample_latents(\n",
    "        batch_size=batch_size,\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        guidance_scale=guidance_scale,\n",
    "        model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "        progress=True,\n",
    "        clip_denoised=True,\n",
    "        use_fp16=True,\n",
    "        use_karras=True,\n",
    "        karras_steps=64,\n",
    "        sigma_min=1e-3,\n",
    "        sigma_max=160,\n",
    "        s_churn=0,\n",
    "    )\n",
    "\n",
    "    for i, latent in enumerate(latents):\n",
    "        t = decode_latent_mesh(xm, latent).tri_mesh()\n",
    "        obj_path = os.path.join('models', f'{modeldesc}.obj')\n",
    "        with open(obj_path, 'w') as f:\n",
    "            t.write_obj(f)\n",
    "        return obj_path\n",
    "\n",
    "@app.route('/model')\n",
    "def get_model():\n",
    "    modeldesc = request.args.get('description', default='a chocolate donut', type=str)\n",
    "    obj_path = get_ai_3d_model(modeldesc)\n",
    "    return send_from_directory(directory='models', path=f'{modeldesc}.obj')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e505a0-1aa5-495b-98ff-42e7aadec3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb400d19-887f-4e2b-ba1a-6cedd5929feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc55710-ac7f-4354-a2f6-bac48e3359ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
